{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwWe8aH__ozk"
      },
      "source": [
        "# Lab 10: Generative AI - Generative Adversarial Networks (GANs)\n",
        "\n",
        "Objectives:\n",
        "1.  **GAN Theory:** An intuitive explanation of how Generative Adversarial Networks work.\n",
        "2.  **Implementation:** Building a simple GAN from scratch using PyTorch to generate images of handwritten digits (MNIST dataset).\n",
        "3. Recognize common GAN failure modes: **mode collapse**, **discriminator overpowering the generator**, and **vanishing gradients for the generator**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aSb0Tby_ozn"
      },
      "source": [
        "## 1. Theory of Generative Adversarial Networks (GANs)\n",
        "\n",
        "GANs are a clever type of generative model introduced by Ian Goodfellow and his colleagues in 2014. The core idea is to have two neural networks compete against each other in a zero-sum game. This competition drives both networks to improve until the model can generate new, realistic data.\n",
        "\n",
        "The two networks are:\n",
        "\n",
        "1.  **The Generator (G):** Its job is to create fake data. It takes a random noise vector (called a latent vector) as input and tries to transform it into something that looks like the real data (e.g., an image of a handwritten digit).\n",
        "\n",
        "2.  **The Discriminator (D):** Its job is to be a detective. It takes in data (both real samples from the training set and fake samples from the Generator) and must determine whether the data is real or fake. It outputs a probability between 0 (definitely fake) and 1 (definitely real).\n",
        "\n",
        "### The Adversarial Game\n",
        "\n",
        "Think of the Generator as an art forger trying to create counterfeit paintings, and the Discriminator as an art critic trying to identify them.\n",
        "\n",
        "*   Initially, the forger (Generator) is terrible and produces random scribbles. The critic (Discriminator) can easily tell the fakes from the real paintings.\n",
        "*   The forger gets feedback from the critic's decisions and learns to improve its forgeries to better fool the critic.\n",
        "*   As the forger gets better, the critic must also get better at spotting the increasingly sophisticated fakes.\n",
        "\n",
        "This process repeats. The Generator's goal is to **maximize** the final error (fool the Discriminator), while the Discriminator's goal is to **minimize** the final error. They are trained in an alternating fashion. By the end of the training, the Generator becomes so good that the Discriminator is only about 50% accurate, meaning it can no longer distinguish real from fake. At this point, the Generator can produce highly realistic, novel data.\n",
        "\n",
        "![GAN Architecture](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQhpV1m2IAZRcvdwkhVak3MpWEeWVrpArLxtA&s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_epHkCr_ozn"
      },
      "source": [
        "## 2. Implementation: A Simple GAN for MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hba8Kevn_ozo",
        "outputId": "6e158b77-56a0-43cf-c397-d83e82ce4488"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8JUqoIh_ozo"
      },
      "source": [
        "### 2.1 Hyperparameters and Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zUYgv2W_ozp",
        "outputId": "0dad3ea3-8c9f-4c54-b886-dc0814cdb594"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.49MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 131kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.24MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.41MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(60000, 468)"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Baseline hyperparameters\n",
        "latent_dim = 64\n",
        "hidden_dim = 256\n",
        "image_size = 28 * 28  # MNIST flattened\n",
        "batch_size = 128\n",
        "lr_g = 2e-4\n",
        "lr_d = 2e-4\n",
        "\n",
        "beta1, beta2 = 0.5, 0.999\n",
        "num_epochs = 30\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # scale to [-1, 1]\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "\n",
        "len(train_dataset), len(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDWpVqfd_ozp"
      },
      "source": [
        "### 2.2 Building the Generator and Discriminator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKDRlKGvGXwT"
      },
      "source": [
        "- **Generator**: noise (latent_dim) → fully connected layers → 784-dim vector → reshaped into 28×28 image.\n",
        "- **Discriminator**: 784-dim vector → fully connected layers → scalar probability via Sigmoid.\n",
        "\n",
        "This is not state-of-the-art (DCGAN with convolutions performs better), but it is easy to understand.\n",
        "\n",
        "You may use LeakyReLU as an activation function for in between layers and Tanh to get the output in [-1,1] interval\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcP7Tcvm_ozp"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim=64, hidden_dim=256, img_dim=784):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            ...\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z)\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, img_dim=784, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(img_dim, hidden_dim),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            ...\n",
        "            nn.Sigmoid() # we want it to guess if it is fake (0) or not (1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEZYvIuN_ozq"
      },
      "source": [
        "### 2.3 Initializing Models, Loss, and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9hyraUR_ozq"
      },
      "outputs": [],
      "source": [
        "G = Generator(latent_dim=latent_dim, hidden_dim=hidden_dim, img_dim=image_size).to(device)\n",
        "D = Discriminator(img_dim=image_size, hidden_dim=hidden_dim).to(device)\n",
        "\n",
        "criterion = ...\n",
        "\n",
        "optimizer_G = ...\n",
        "optimizer_D = ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm5s86jCHGaz"
      },
      "outputs": [],
      "source": [
        "#Visualizing generated samples helper\n",
        "def show_fake_samples(epoch, fixed_noise, nrow=8):\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        fake = G(fixed_noise.to(device)).view(-1, 1, 28, 28)\n",
        "    G.train()\n",
        "    fake = (fake + 1) / 2.0  # back to [0, 1] for display\n",
        "    grid = utils.make_grid(fake, nrow=nrow)\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.title(f\"Generated samples at epoch {epoch}\")\n",
        "    plt.axis('off')\n",
        "    plt.imshow(grid.cpu().permute(1, 2, 0).squeeze())\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "fixed_noise = torch.randn(64, latent_dim, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfl09UX7HXDg"
      },
      "outputs": [],
      "source": [
        "show_fake_samples(0, fixed_noise)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGhBuE1D_ozq"
      },
      "source": [
        "### 2.4 The Training Loop\n",
        "We first train a **baseline GAN** with reasonably stable hyperparameters.  \n",
        "After that, we will **intentionally break** the training in different ways to illustrate:\n",
        "\n",
        "- **Mode collapse**\n",
        "- **Discriminator overpowering the Generator**\n",
        "- **Vanishing gradients for the Generator**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3s4Cvwzu_ozq"
      },
      "outputs": [],
      "source": [
        "G.train()\n",
        "D.train()\n",
        "\n",
        "g_losses = []\n",
        "d_losses = []\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    for real_imgs, _ in train_loader:\n",
        "        real_imgs = real_imgs.view(-1, image_size).to(device)\n",
        "\n",
        "        # --------------------\n",
        "        #  Train Discriminator\n",
        "        # --------------------\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        # Prepare real and fake labels for future train\n",
        "        real_labels = ... # array of 1s for the selected images\n",
        "        fake_labels = ... # array of 0s for the to-be-generated images\n",
        "\n",
        "        outputs_real = D(real_imgs)\n",
        "        d_loss_real = criterion(outputs_real, real_labels)\n",
        "\n",
        "        # Fake images\n",
        "        z = torch.randn(real_imgs.size(0), latent_dim, device=device)\n",
        "        fake_imgs = ... # generate from noise\n",
        "        outputs_fake = ... # what does the discriminator think?\n",
        "        d_loss_fake = criterion(outputs_fake, fake_labels)\n",
        "\n",
        "        d_loss = d_loss_real + d_loss_fake\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        # ----------------\n",
        "        #  Train Generator\n",
        "        # ----------------\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        z = torch.randn(real_imgs.size(0), latent_dim, device=device)\n",
        "        fake_imgs = G(z)\n",
        "        outputs = D(fake_imgs)\n",
        "        # Generator wants D to think fakes are real (label = 1)\n",
        "        g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "        # backward and steps...\n",
        "        # ...\n",
        "\n",
        "        g_losses.append(g_loss.item())\n",
        "        d_losses.append(d_loss.item())\n",
        "\n",
        "    # clear_output(wait=True)\n",
        "    print(f\"Epoch [{epoch}/{num_epochs}]  D_loss: {d_loss.item():.4f}  G_loss: {g_loss.item():.4f}\")\n",
        "    show_fake_samples(epoch, fixed_noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shU07Jwo_ozq"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(g_losses, label='G loss')\n",
        "plt.plot(d_losses, label='D loss')\n",
        "plt.xlabel('Training step')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Generator and Discriminator losses (baseline)')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCHyvx9dJmJt"
      },
      "source": [
        "Let's move the above in a function so we can see how it behaves with different hyperparams"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDq3yHU8Jlia"
      },
      "outputs": [],
      "source": [
        "def train_gan(G, D, optimizer_G, optimizer_D, train_loader,\n",
        "              latent_dim, num_epochs,\n",
        "              experiment_name : str,\n",
        "              train_D_twice=False):\n",
        "  G.train()\n",
        "  D.train()\n",
        "\n",
        "  g_losses = []\n",
        "  d_losses = []\n",
        "\n",
        "  fixed_noise = torch.randn(64, latent_dim, device=device)\n",
        "\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "      for real_imgs, _ in train_loader:\n",
        "          real_imgs = real_imgs.view(-1, image_size).to(device)\n",
        "\n",
        "          # --------------------\n",
        "          #  Train Discriminator\n",
        "          # --------------------\n",
        "          # For enhancing the misbalance that can happen with a discriminator\n",
        "          # that is too good, we allow it to train multiple times\n",
        "          d_steps = 1 if train_D_twice == False else 2\n",
        "          for _ in range(d_steps):\n",
        "            ...\n",
        "          # ----------------\n",
        "          #  Train Generator\n",
        "          # ----------------\n",
        "          ...\n",
        "          # Generator wants D to think fakes are real (label = 1)\n",
        "          g_loss = criterion(outputs, real_labels)\n",
        "\n",
        "          # optimizers, steps...\n",
        "          ...\n",
        "          g_losses.append(g_loss.item())\n",
        "          d_losses.append(d_loss.item())\n",
        "\n",
        "      print(\n",
        "            f\"Mode: {experiment_name} | Epoch [{epoch}/{num_epochs}] \"\n",
        "            f\"D_loss: {d_loss.item():.4f}  G_loss: {g_loss.item():.4f}\"\n",
        "      )\n",
        "      show_fake_samples(epoch, fixed_noise, nrow=8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyDIuQz_H_B_"
      },
      "source": [
        "## 6. Failure Mode Showcase: Breaking the GAN on Purpose\n",
        "\n",
        "In this section, we **intentionally misconfigure** the GAN to demonstrate three classic failure modes:\n",
        "\n",
        "1. **Mode collapse** - the Generator produces very similar or identical samples.\n",
        "2. **Discriminator overpowered by the Generator/Generator overpowering Discriminator**\n",
        "3. **Vanishing gradients for the Generator** - G's gradients go to ~0, training stalls.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJx19ZOSQTFF"
      },
      "source": [
        "### Mode Collapse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjIpGi4aQUfo"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xojbk3IpNYLT"
      },
      "source": [
        "### Discriminator overpowering the Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PAWSG6ruLvgk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
